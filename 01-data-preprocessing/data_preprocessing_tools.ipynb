{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# **Data Preprocessing Tools**\n",
    "<hr>\n",
    "\n",
    "- Techniques used to improve the quality of our data\n",
    "- To remove or to minimize the problems with noises, outliers, incorrect values, inconsistent, duplicates, or absents.\n",
    "- Turn the data more adequate to a certain algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## **Importing the libraries**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **numpy** library allow us to work with fast arrays and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **matplotlib** allow us to plot very nice charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas** allow us to import the dataset and create the matrix of features and the dependent variable vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## **Importing the dataset**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"Data.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country       object\n",
       "Age          float64\n",
       "Salary       float64\n",
       "Purchased     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any dataset which we're going to train a machine learning projetct, we have the same entities.\n",
    "- **Matrix of Features:** The features are the columns (independent variables) which you're going to predict the dependent variable vector. \n",
    "    - **Note**: A great tip to select the features that will be used to train our model is to remove the variables that we have the **assurance** that won't influence the model, like the person name, and make correlations with all attributes, if we have 2 attributes high correlated (more than 0.66 or 0.8), we can remove one of these attributes, since that indicates data redundancy. Furthermore, it can happen that one independent attribute is highly correlated with the dependent variable, and just with this independent variable we can predict the dependent variable but pay attention, correlation does not indicate causality.\n",
    "\n",
    "\n",
    "- **Dependent Variable Vector:** In this dataset, this variable is the last column, because this company would like to predict if future clients will purchase or will not based, on their information. \n",
    "    - **Note**: In supervised learning we have to pay attention to the balance of the classes to predict, it's not good for the model to be trained with a dependent variable with 95%  of one class, and 5% of another class, in this case we have to do a balance of our data using some techniques. So, descritive statistics is very important.\n",
    "    \n",
    "**Select the right data** to traing our model is very important, we have to know a lot about the data that we're working. Once present the best experience to the machine, or the best data, finally we can expect solutions with the **minimum human bias**. So, whether the data passed to our model training is biased, our model will be biased too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our **Matrix of Features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary\n",
       "0   France  44.0  72000.0\n",
       "1    Spain  27.0  48000.0\n",
       "2  Germany  30.0  54000.0\n",
       "3    Spain  38.0  61000.0\n",
       "4  Germany  40.0      NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.iloc[:, :-1] #Slicing the dataframe by the indexes -> df.iloc[rows, columns]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.values\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our **Dependent Variable Vector**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Purchased\n",
       "0        No\n",
       "1       Yes\n",
       "2        No\n",
       "3        No\n",
       "4       Yes"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:, -1:]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['No'],\n",
       "       ['Yes'],\n",
       "       ['No'],\n",
       "       ['No'],\n",
       "       ['Yes'],\n",
       "       ['Yes'],\n",
       "       ['No'],\n",
       "       ['Yes'],\n",
       "       ['No'],\n",
       "       ['Yes']], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "<br>\n",
    "\n",
    "## **Taking care of missing data**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have look at our df, we noticed that there is a missing salary and age value. So, generally we don't want to have any missing data in our dataset, because it can cause some erros when we are training our ML model and therefore we must handle them. Thus we have some ways to handle missing data.\n",
    "\n",
    "**Ways to handle missing data**:\n",
    "\n",
    "1. Just ignore that observation by deleting it. It's a good option if we have a large dataset and just 1% of missing data. Since that deleting 1% of a large dataset won't change much the learning quality of our model.\n",
    "\n",
    "2. Replace the missing value by the average of all values in the column in which the data is missing. This is a classic way of handling missing data.\n",
    "    - **Note:** It is a good strategy but not the best one always, for example, we mustn't put a mean weight of our dataset for a baby weight in a dataset where the most part is adult. So, it **depends on your business problem, on the way your data is distributed and on the number of missing values**. If for example you have a lot of missing values, then mean substitution is not the best thing. Other strategies include \"median\" imputation, \"most frequent\" imputation or prediction imputation. Prediction Imputation is actually another great strategy that is recommended but we didn't learn yet. Once we learn this, using the k-NN algorithm is a good choice for this situation, but this could possibly be adding inaccuracy to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn library we have a tool to replace easily the missing value by the average or median or the most frequent value of all values in the column called SimpleImputer. In this case, we gonna replace by the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") #strategy can be \"mean\", \"median\" or \"most_frequent‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fit** method looks to the missing values and computes the average, when called we must have to pass only numerical columns as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(x[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **transform** method applies the transformation in our data and return it, we must have to pass the same columns passed as argument in the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 1:3] = imputer.transform(x[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the **fit** part is used to extract some info of the data on which the object is applied (here, Imputer will spot the missing values and get the mean of the column). Then, the **transform** part is used to apply some transformation (here, Imputer will replace the missing value by the mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## **Encoding categorical data**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, our dataset can contain categorical columns. In the current dataset that we are working on, we have the column *Country* which has categories, like \"France\", \"Spain\", or \"Germany\". This type of data will difficult for a ML Model to compute some correlations between the features and the dependent variable. \n",
    "\n",
    "To solve this, we can't encode the categories in number, like \"France\" = 0, \"Spain\" = 1, and \"Germany\" = 2, because has a numerical order, due to this, it could interpret that this order matters and of course it is absolutely wrong in this case. Therefore, we can actually do much better than just encode these three into zero, one, and two. \n",
    "\n",
    "To make this much better we will use the *OneHotEncoding* that consists of turning the country column into three new columns because there are three different categorical values in this column. If the first column it's equal to one, so the country of this row is \"France\", if the second one it's equal to one, the country of the row is \"Spain\", and the same thing to the third column.\n",
    "\n",
    "Now, let's talk about the dependent variable. In this case, where the dependent variable is a binary value, just can be \"Yes\" or \"No\", we can replace it with zero and one. Doing this in a binary dependent variable won't compromise the future accuracy of the model if we just replace it.\n",
    "\n",
    "**Note:** In other words, if we have order in our categorical variable or is it a binary value we can use LabelEncoder, if not OneHotEncoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### **Encoding the Independent Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "                      #transformers = [(type, transformer, columns)], remainder= \"drop\" or \"passthrough\" \n",
    "ct = ColumnTransformer(transformers=[(\"encoder\", OneHotEncoder(), [0])], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method **fit_transform** receive as an argument the data that will be encoded and return encoded. Once returned, we have to convert to a numpy array. An encoded variable is called too as dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(ct.fit_transform(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### **Encoding the Dependent Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() #it transforms the categories strings into integers\n",
    "# ravel will convert that array shape to (n, )\n",
    "y = le.fit_transform(y.ravel()) #it's not obligatory to pass a numpy array dependent variable to our future models\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## **Splitting the dataset into the Training set and Test set**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the dataset into the Training set and Test set** consists of making two separate sets, one training set where we're going to train our ML Model on existing observations and one test set where you're going to evaluate the performance of our model on new observations. In this case, new observations are making the paper of some future observations that we're going to get when we're going to deploy our ML Model.\n",
    "\n",
    "**Feature Scaling** simply consists of scaling all our features to make sure they all take values on the same scale. We do this to prevent that one feature dominates another, which therefore would be neglected by the ML Model. \n",
    "\n",
    "Due to this, we have to **split the dataset before applying feature scaling**, because we are preparing our data for the model, and *when we put new observations it will be completely new to the model*. If we do the scaling feature before the split, the model will be influenced by the test data observations, so we are doing something like **information leakage** or **data leakage** and when we are going to evaluate our model the new data won't be as new as we thought.\n",
    "\n",
    "**Data leakage** in machine learning happens when the data that we are used to training a machine learning algorithm is having the information which the model is trying to predict, this results in unreliable and bad prediction outcomes after model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=1/5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to separate 1/3 or 1/5 of our data to test the ML model, depending on the size of the dataset. The random state guarantees that we will always have the same random factor during the split. The data split will happen randomly. **Pay attention**, it can happen that one training sample does not represent the problem that we want to modelate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0]], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## **Feature Scaling**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling allows us to put all our features on the same scale. We need to do this to prevent that one feature dominates another, which therefore would be neglected by the ML Model. Furthermore, we need to be aware that we won't have to apply feature scaling for all ML Models, just for some of them. Actually, generally, we apply this technique if we have features taking very different values.\n",
    "\n",
    "How do we do feature scaling?\n",
    "\n",
    "The main two feature scaling techniques are **Standardisation** and **Normalisation**.\n",
    "\n",
    "- **Standardisation**\n",
    "    - When applied all the features will take values between -3 and +3;\n",
    "    - Actually works well all the time, it will do the job all the time;\n",
    "\n",
    "$$\n",
    "x_{stand}= \\frac{x - mean(x)}{standard \\ deviation(x)}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Normalisation**\n",
    "    - Since the numerator and denominator are positive. Furthermore, the denominator will be always greater than the numerator, the features will take values between 0 and 1;\n",
    "    - It's recommended when we have a normal distribution in most of our features;\n",
    "\n",
    "$$\n",
    "x_{norm} = \\frac{x - min(x)}{max(x) - min(x)}\n",
    "$$\n",
    "\n",
    "Let's coding it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have we to apply feature scaling to dummy variables?**\n",
    "\n",
    "No, because the feature scaling aims to put all features in the same range, and when we apply standardisation all features will take values between -3 and 3, but the dummy variables are 0 and 1, in other words, they already are between -3 and 3. Indeed, standadisation will only make it worse, because it will transform these values between -3 and 3, and making this we lose the interpretation of the variables. In this dataset, if we apply feature scaling in dummy variables, we will lose the information of which country corresponds to the observation. However, we have to apply the feature scaling to the age and salary columns, since we have a wide range of values. But even so, despite losing the interpretation of our data, in some cases, apply feature scaling to dummy variables can optimize the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, -0.19159184384578545, -1.0781259408412425],\n",
       "       [0.0, 1.0, 0.0, -0.014117293757057777, -0.07013167641635372],\n",
       "       [1.0, 0.0, 0.0, 0.566708506533324, 0.633562432710455],\n",
       "       [0.0, 0.0, 1.0, -0.30453019390224867, -0.30786617274297867],\n",
       "       [0.0, 0.0, 1.0, -1.9018011447007988, -1.420463615551582],\n",
       "       [1.0, 0.0, 0.0, 1.1475343068237058, 1.232653363453549],\n",
       "       [0.0, 1.0, 0.0, 1.4379472069688968, 1.5749910381638885],\n",
       "       [1.0, 0.0, 0.0, -0.7401495441200351, -0.5646194287757332]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the test data, we have to make the scaling feature with the same standardisation scale (mean and std) used in the pre-processing of the model trained, to make this, we just call the method **transform** of the object used to fit the standardisation scale in our train data. Once that when we are going to apply new data in our model, we will have just one observation with a different range of values, so we have to put in the same range of our training data, but without making the fit, seeking not to disturb the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, -1.4661817944830124, -0.9069571034860727],\n",
       "       [1.0, 0.0, 0.0, -0.44973664397484414, 0.2056403393225306]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])\n",
    "X_test"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
